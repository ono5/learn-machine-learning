# 教師なし学習

## パラメータとハイパーバラメータ
* パラメータ
	- モデルの学習実行後に獲得される値(wやσなど)
* ハイパーパラメータ
	- 各アルゴリズムに付随して、アルゴリズムの挙動を制御するための値
	- モデルの学習実行前に調整することでモデルの性能向上や過学習の抑制、効率の良い学習などが期待できる

## K-分割交差検証 (K-fold cross-validation)
ホールドアウト法では、与えられたデータセットを学習用データセット・テスト用データセットを 2 分割したが、
実際の開発時にはモデルの性能評価をより適切にするためにデータを 3 分割してモデルを評価することが一般的。

|データ名称|使用目的|
|:--|:--|
|学習用データセット (train)	|モデルを学習させるためのデータセット|
|検証用データセット (validation)|ハイパーパラメータの調整が適切なのか検証するためのデータセット|
|テスト用データセット (test)|学習済みモデルの性能を評価するためのデータセット|
* 学習用データセットと検証用データセットは学習段階で用いられ、テスト用データセットは最終的なモデルの予測精度の確認のためにのみ使用する

しかし、十分なデータ量が用意できない場合には 3 分割すると偏りが生じて適切な学習・検証が行われない可能性があり、
そのようなデータの偏りを回避する方法として `K-分割交差検証 (K-fold cross-validation)`が用いられる。

<3 Step>
1. データセットを `K` 個に分割
2. 分割したデータの 1 個を検証用データセットとし、残り `K-1` 個を学習用データセットとして学習を実行
	*  1 回で学習を終わらせず、計 K 回の学習を実施する
	* 既に検証用データセットに使ったデータを次は学習用データセットとして使用し、新たに検証用データセットを選択する
3. 各検証の結果を平均して最終的な検証結果とする

## ハイパーパラメータの調整方法
以下の方法を使って、ハイパーパラメータを調整する。

1. 手動での調整
2. グリッドサーチ (Grid Search)
3. ランダムサーチ (Random Search)
4. ベイズ最適化 (Bayesian Optimization

### 事前準備
```
import numpy as np
import pandas as pd
from sklearn.datasets import load_breast_cancer

# 乳がんに関するデータセット
# 目標値が陰性か陽性かの 2 つの値である二値分類の問題設定
dataset = load_breast_cancer()

t = dataset.target
x = dataset.data

print(x.shape, t.shape)
```

```
(569, 30) (569,)
```

### 手動での調整

```
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier

# 学習用データセット・検証用データセット・テスト用データセットの 3 つに分割

# 与えられたデータを「テスト用データセット：その他 ＝ 20 ： 80 」に分割
x_train_val, x_test, t_train_val, t_test = train_test_split(x, t, test_size=0.2, random_state=1)

# 「その他」のデータを「検証用データセット：学習用データセット ＝ 30 ： 70 」に分割
# 検証用データセット：学習用データセット＝ 30 ： 70
x_train, x_val, t_train, t_val = train_test_split(x_train_val, t_train_val, test_size=0.3, random_state=1)

print(x_train.shape, x_val.shape, x_test.shape)

# 決定木で学習
dtree = DecisionTreeClassifier(random_state=0).fit(x_train, t_train)

# ハイパーパラメータを設定して、モデルの定義
dtree2 = DecisionTreeClassifier(max_depth=10, min_samples_split=30, random_state=0).fit(x_train, t_train)

# スコア確認
print('train score(前) : ', dtree.score(x_train, t_train))
print('train score(後) : ', dtree2.score(x_train, t_train))
print('validation score(前) : ', dtree.score(x_val, t_val))
print('validation score(後) : ', dtree2.score(x_val, t_val))
print('test score(前) :', dtree.score(x_test, t_test))
print('test score(後) :', dtree2.score(x_test, t_test))
```

```
(318, 30) (137, 30) (114, 30)
train score(前) :  1.0
train score(後) :  0.9308176100628931
validation score(前) :  0.927007299270073
validation score(後) :  0.9562043795620438
test score(前) : 0.9210526315789473
test score(後) : 0.9298245614035088
```

### グリッドサーチ (Grid Search)
最適なハイパーパラメータを獲得するにはある程度の探索（試行錯誤）を行う必要がある。

グリッドサーチは、`効率的に最適なハイパーパラメータを探索する方法`の一つ。

決定木のハイパーパラメータを例に説明する。

1. ハイパーパラメータを探索する範囲を決める
	- `max_depth` と `min_samples_split` の値を調整したい場合、`5、10、15、20、25` のように範囲をそれぞれ決める
	- パイパーパラメータは、5 * 5 = 25の組み合わせになる
2. 全てのパイパーパラメータの組み合わせを使用して、学習・検証を行う
3. 結果から予測精度が最も高いハイパーパラメータを採用する

* メリット：指定した範囲を網羅するため、ある程度漏れがなくハイパーパラメータの探索を行うことができる
* デメリット：場合によっては、数十～数百パターンの組合せを計算するため学習に時間を要する

```
# GridSearchCV クラスのインポート
from sklearn.model_selection import GridSearchCV

# GridSearchCV クラスの使用には下記の 3 つを準備する必要がある
# estimator ：学習に使用するモデル
estimator = DecisionTreeClassifier(random_state=0)
# param_grid ：ハイパーパラメータを探索する範囲
param_grid = [{
    'max_depth': [3, 20, 50],
    'min_samples_split': [3, 20, 30]
}]
# cv ：K-分割交差検証の K の値
cv = 5

# K-分割交差検証を使いたいため、学習用データセットと検証用データセットに分割する前の
# データセットである x_train_val と t_train_val を使用
# return_train_score=False を設定することで学習に対する予測精度の検証が行われない
# False にすると計算コストを抑えることができる
tuned_model = GridSearchCV(estimator=estimator,
                           param_grid=param_grid,
                           cv=cv, return_train_score=False
						   ).fit(x_train_val, t_train_val)

# 検証結果の確認
# ハイパーパラメータの種類が 2 つで、各 3 個ずつ値を指定したので 3 × 3 = 9 パターンの計算が行われる
# K を 5 としたので 5 種類の結果 (split0_test_score ~ split4_test_score) が出力
print(pd.DataFrame(tuned_model.cv_results_).T)
```

```
mean_fit_time                                              0.007147
std_fit_time                                               0.000441
mean_score_time                                            0.000821
std_score_time                                             0.000045
param_max_depth                                                  50
param_min_samples_split                                          30
params                   {'max_depth': 50, 'min_samples_split': 30}
split0_test_score                                          0.912088
split1_test_score                                          0.901099
split2_test_score                                          0.934066
split3_test_score                                          0.945055
split4_test_score                                          0.901099
mean_test_score                                            0.918681
std_test_score                                             0.017855
rank_test_score                                                   6
...
```

`mean_test_score`の値からそのモデルの予測精度の確認ができる。
基本的にはこの値を確認し、どのハイパーパラメータが効果が強いのかを確認する。

基本的にパラメータの変更を繰り返して、最適な値を探る。

```
estimator = DecisionTreeClassifier(random_state=0)
cv = 5
param_grid = [{
    'max_depth': [5, 10, 15] ,
    'min_samples_split': [10, 12, 15]
}]
# モデルの定義
tuned_model = GridSearchCV(estimator=estimator,
                           param_grid=param_grid,
                           cv=cv, return_train_score=False)

# モデルの学習
tuned_model.fit(x_train_val, t_train_val)
# 学習結果の確認
pd.DataFrame(tuned_model.cv_results_).T
```

最初はある程度大きな幅を持ってグリッドサーチを行い、徐々に範囲を狭めてより予測精度の高いハイパーパラメータを探していく。

```
# 最も予測精度の高かったハイパーパラメータの確認
tuned_model.best_params_
```

```
{'max_depth': 20, 'min_samples_split': 3}
```

best_estimator_ で最も検証用データセットに対しての予測精度が最も高かったハイパーパラメータで
学習したモデルを取得することが可能。

```
# 最も予測精度の高かったモデルの引き継ぎ
best_model = tuned_model.best_estimator_

# モデルの検証
print(best_model.score(x_train_val, t_train_val))
print(best_model.score(x_test, t_test))
```

```
1.0
0.9385964912280702
```

### ランダムサーチ (Random Search)
### ベイズ最適化 (Bayesian Optimization)
