# 分類
分類は、カテゴリが異なる複数のデータを見分けることができる境界線を求めることが目的。

二次元平面上にあるデータ（集合）を一本の直線で分けられることを線形分離可能といい、
アルゴリズムを線形分類器と呼ぶ。

なお、線形ではない形で分類するアルゴリズムを非線形分類器と呼ぶ。

## 決定木(非線形分類器)
決定木の score() メソッドで表示される値には`正解率 （Accuracy）`が用いられる。
(分類の場合は、決定係数が求まる)

例えば、100 個の要素に対して分類を行い 90 個の予測を正しく行えた場合、
正解率は 90%(0.9) になる。

そのため最小の値は 0 となり、最大値は 1 です。

また、回帰では平均二乗誤差が用いられるが、分類では交差エントロピーが主に用いられる。

### 実装
```
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import load_iris

# データセットの読み込み
dataset = load_iris()
x = dataset.data
t = dataset.target
feature_names = dataset.feature_names

# 学習データとテストデータに分割
from sklearn.model_selection import train_test_split
x_train, x_test, t_train, t_test = train_test_split(x, t, test_size=0.3, random_state=0)

# モデルの定義
from sklearn.tree import DecisionTreeClassifier
dtree = DecisionTreeClassifier(random_state=0)

# モデルの学習
dtree.fit(x_train, t_train)

# モデルの検証
print('train score : ', dtree.score(x_train, t_train))
print('test score : ', dtree.score(x_test, t_test))

# 推論
dtree.predict(x_test)
```

## 特徴
[決定木](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier)は木のような構造を用いて、回帰・分類を行うアルゴリズム。

データの標準化や外れ値除去に関しては決定木では不要であるなどメリットがある。


|項目|説明|
|:--|:--|
|強み|解釈が容易。必要な前処理が少ない。|
|弱み|過学習になる場合が多く、汎用性の低いモデルになる傾向がある。|
|主なハイパーパラメータ||
|max_depth(木構造の深さの上限)|過学習を抑えるためのハイパーパラメータ。上限が低いとモデルの表現力は低下し、過学習を抑える。|
|min_samples_split(木構造の分岐先の値)|分岐先のノード数の最低値を設定するハイパーパラメータ。過学習に陥る可能性が上がるので調整が必要。|

### 木構造の書き出し

```
# 木構造の書き出し
import graphviz
from sklearn.tree import export_graphviz
dot_data = export_graphviz(dtree)

# 木構造の表示
graph_tree = graphviz.Source(dot_data)
graph_tree
```
<img width="732" alt="スクリーンショット 2022-10-16 21 26 00" src="https://user-images.githubusercontent.com/20691160/196035346-0971ac0a-3701-4fb6-8da2-01447c09b240.png">

gini という表記はジニ係数を意味しており、分岐されたノードの不純度を表す。
※ もしも、カテゴリが 1 つしか存在しない場合、ジニ係数は 0。

### 入力変数の影響度
アルゴリズムの特性からどの入力変数の影響度が高いかを知ることが可能。

入力変数の影響度を知るには、`feature_importances_`を確認する。

```
# feature importance
feature_importance = dtree.feature_importances_
feature_importance
```

### 可視化
可視化を行うことで、入力変数の影響が大きいかチェックできて便利.

```
# 可視化
y = feature_names
width = feature_importance

# 横向きで表示
plt.barh(y=y, width=width);
```

<img width="464" alt="スクリーンショット 2022-10-16 21 32 31" src="https://user-images.githubusercontent.com/20691160/196035561-d9a5662d-4748-495c-9f7c-36daaed00558.png">

 ↑ `petal width (cm)`の影響度が高い

## サポートベクトルマシン(SVM)
サポートベクトルマシンは、2 つのカテゴリを識別する分類器であり、サポートベクターマシンとも呼ばれる。

![スクリーンショット 2022-10-16 21 56 04](https://user-images.githubusercontent.com/20691160/196036625-e0c61f29-1ff7-4ba1-971e-33a46d8b64e3.png)

1. 上記のように未知のデータを求めたい時、青丸と緑三角の中間地点に境界線を引く(サポートベクトル)
2. 境界に近い丸と三角のデータのマージン(距離)が最大になるように線を引く(マージン最大化)


### 実装
サポートベクトルマシンで学習を行う前にデータの`標準化(スケール合わせ)`が必要.


```
# モデルの定義
from sklearn.svm import SVC
svc = SVC()

# モデルの学習
svc.fit(x_train, t_train)

# モデルの検証
print('train score : ', svc.score(x_train, t_train))
print('test score : ', svc.score(x_test, t_test))

# データの標準
# SVMはデータに対して標準化を行う必要がある
# 標準化とは例えば一つのデータの単位がcmだった時他のパラメータのスケールも同じように一致させる
from sklearn.preprocessing import StandardScaler
std_scaler = StandardScaler()

# 標準化の計算式
# ^      _
# X = (x-x) / σ
# 学習(fitで標準化を行うためのデータセットの平均と標準偏差を算出)
std_scaler.fit(x_train)

# 標準化(transformにて、算出した平均と標準偏差から実際にデータセットの値を変換)
x_train_std = std_scaler.transform(x_train)
x_test_std = std_scaler.transform(x_test)

# 平均
print('平均 : ', round(x_train_std.mean()))

# 標準偏差
print('標準偏差 : ', round(x_train_std.std()))

# モデルの定義
svc_std = SVC()

# モデルの学習(標準化したデータを使う)
svc_std.fit(x_train_std, t_train)

# モデルの検証
print('train score : ', svc.score(x_train, t_train))
print('test score : ', svc.score(x_test, t_test))

print('train score scaling : ', svc_std.score(x_train_std, t_train))
print('test score scaling : ', svc_std.score(x_test_std, t_test))
```

```
train score :  0.9714285714285714
test score :  0.9777777777777777
平均 :  0
標準偏差 :  1
train score :  0.9714285714285714
test score :  0.9777777777777777
train score scaling :  0.9714285714285714
test score scaling :  0.9777777777777777
```

### 特徴

|項目|説明|
|:--|:--|
|強み|未知のデータへの識別性能が比較的強い。ハイパーパラメータの数が少ない。|
|弱み|学習する際に必ずデータの標準化（もしくは正規化）を行う必要がある。|
|主なハイパーパラメータ||
|C（コストパラメータ）|誤った予測に対するペナルティ。大き過ぎると過学習を起こす。|
|gamma（ガンマ）|モデルの複雑さを決定する。値が大きくなるほどモデルが複雑になり過学習を起こす。|
