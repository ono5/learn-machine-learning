# 分類
分類は、カテゴリが異なる複数のデータを見分けることができる境界線を求めることが目的。

二次元平面上にあるデータ（集合）を一本の直線で分けられることを線形分離可能といい、
アルゴリズムを線形分類器と呼ぶ。

なお、線形ではない形で分類するアルゴリズムを非線形分類器と呼ぶ。

## Code
* [分類1](https://colab.research.google.com/drive/1ub3nSK73ekCZFqrvcI-DkBLfp98iUtfb)
* [分類2](https://colab.research.google.com/drive/1C7YFmTakVgHkkHqPJRr2rQ97V7YsTzey#scrollTo=2wpJojuGPqB1)

## 決定木(非線形分類器)
決定木の score() メソッドで表示される値には`正解率 （Accuracy）`が用いられる(分類の場合は、決定係数が求まる)。

例えば、100 個の要素に対して分類を行い 90 個の予測を正しく行えた場合、
正解率は 90%(0.9) になる。

そのため最小の値は 0 となり、最大値は 1 です。

また、回帰では平均二乗誤差が用いられるが、分類では交差エントロピーが主に用いられる。

## 特徴
[決定木](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier)は木のような構造を用いて、回帰・分類を行うアルゴリズム。

データの標準化や外れ値除去に関しては決定木では不要であるなどメリットがある。

|項目|説明|
|:--|:--|
|強み|解釈が容易。必要な前処理が少ない。|
|弱み|過学習になる場合が多く、汎用性の低いモデルになる傾向がある。|
|主なハイパーパラメータ||
|max_depth(木構造の深さの上限)|過学習を抑えるためのハイパーパラメータ。上限が低いとモデルの表現力は低下し、過学習を抑える。|
|min_samples_split(木構造の分岐先の値)|分岐先のノード数の最低値を設定するハイパーパラメータ。過学習に陥る可能性が上がるので調整が必要。|


### 実装
```
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import load_iris

# データセットの読み込み
dataset = load_iris()
x = dataset.data
t = dataset.target
feature_names = dataset.feature_names

# 学習データとテストデータに分割
from sklearn.model_selection import train_test_split
x_train, x_test, t_train, t_test = train_test_split(x, t, test_size=0.3, random_state=0)

# モデルの定義
from sklearn.tree import DecisionTreeClassifier
dtree = DecisionTreeClassifier(random_state=0)

# モデルの学習
dtree.fit(x_train, t_train)

# モデルの検証
print('train score : ', dtree.score(x_train, t_train))
print('test score : ', dtree.score(x_test, t_test))

# 推論
dtree.predict(x_test)
```

### 木構造の書き出し

```
# 木構造の書き出し
import graphviz
from sklearn.tree import export_graphviz
dot_data = export_graphviz(dtree)

# 木構造の表示
graph_tree = graphviz.Source(dot_data)
graph_tree
```
<img width="732" alt="スクリーンショット 2022-10-16 21 26 00" src="https://user-images.githubusercontent.com/20691160/196035346-0971ac0a-3701-4fb6-8da2-01447c09b240.png">

gini という表記はジニ係数を意味しており、分岐されたノードの不純度を表す。
※ もしも、カテゴリが 1 つしか存在しない場合、ジニ係数は 0。

### 入力変数の影響度
アルゴリズムの特性からどの入力変数の影響度が高いかを知ることが可能。

入力変数の影響度を知るには、`feature_importances_`を確認する。

```
# feature importance
feature_importance = dtree.feature_importances_
feature_importance
```

### 可視化
可視化を行うことで、入力変数の影響が大きいかチェックできて便利.

```
# 可視化
y = feature_names
width = feature_importance

# 横向きで表示
plt.barh(y=y, width=width);
```

<img width="464" alt="スクリーンショット 2022-10-16 21 32 31" src="https://user-images.githubusercontent.com/20691160/196035561-d9a5662d-4748-495c-9f7c-36daaed00558.png">

 ↑ `petal width (cm)`の影響度が高い

## サポートベクトルマシン(SVM)
サポートベクトルマシンは、2 つのカテゴリを識別する分類器であり、サポートベクターマシンとも呼ばれる。

![スクリーンショット 2022-10-16 21 56 04](https://user-images.githubusercontent.com/20691160/196036625-e0c61f29-1ff7-4ba1-971e-33a46d8b64e3.png)

1. 上記のように未知のデータを求めたい時、青丸と緑三角の中間地点に境界線を引く(サポートベクトル)
2. 境界に近い丸と三角のデータのマージン(距離)が最大になるように線を引く(マージン最大化)

### 特徴

|項目|説明|
|:--|:--|
|強み|未知のデータへの識別性能が比較的強い。ハイパーパラメータの数が少ない。|
|弱み|学習する際に必ずデータの標準化（もしくは正規化）を行う必要がある。|
|主なハイパーパラメータ||
|C（コストパラメータ）|誤った予測に対するペナルティ。大き過ぎると過学習を起こす。|
|gamma（ガンマ）|モデルの複雑さを決定する。値が大きくなるほどモデルが複雑になり過学習を起こす。|

### 実装
サポートベクトルマシンで学習を行う前にデータの`標準化(スケール合わせ)`が必要.

```
# モデルの定義
from sklearn.svm import SVC
svc = SVC()

# モデルの学習
svc.fit(x_train, t_train)

# モデルの検証
print('train score : ', svc.score(x_train, t_train))
print('test score : ', svc.score(x_test, t_test))

# データの標準
# SVMはデータに対して標準化を行う必要がある
# 標準化とは例えば一つのデータの単位がcmだった時他のパラメータのスケールも同じように一致させる
from sklearn.preprocessing import StandardScaler
std_scaler = StandardScaler()

# 標準化の計算式
# ^      _
# X = (x-x) / σ
# 学習(fitで標準化を行うためのデータセットの平均と標準偏差を算出)
std_scaler.fit(x_train)

# 標準化(transformにて、算出した平均と標準偏差から実際にデータセットの値を変換)
x_train_std = std_scaler.transform(x_train)
x_test_std = std_scaler.transform(x_test)

# 平均
print('平均 : ', round(x_train_std.mean()))

# 標準偏差
print('標準偏差 : ', round(x_train_std.std()))

# モデルの定義
svc_std = SVC()

# モデルの学習(標準化したデータを使う)
svc_std.fit(x_train_std, t_train)

# モデルの検証
print('train score : ', svc.score(x_train, t_train))
print('test score : ', svc.score(x_test, t_test))

print('train score scaling : ', svc_std.score(x_train_std, t_train))
print('test score scaling : ', svc_std.score(x_test_std, t_test))
```

```
train score :  0.9714285714285714
test score :  0.9777777777777777
平均 :  0
標準偏差 :  1
train score :  0.9714285714285714
test score :  0.9777777777777777
train score scaling :  0.9714285714285714
test score scaling :  0.9777777777777777
```

## ロジスティック回帰
ロジスティック回帰 (Logistic regression) は、あるデータがカテゴリに属する確率を予測する。

ロジスティック回帰には[シグモイド関数(活性化関数)](https://ja.wikipedia.org/wiki/%E3%82%B7%E3%82%B0%E3%83%A2%E3%82%A4%E3%83%89%E9%96%A2%E6%95%B0)が使われ、あらゆる入力値を`0.0～1.0`の範囲の数値に変換して出力する。

ロジスティック回帰は入力変数から出力変数に対して二値分類をおこなうモデルであり、
3 クラス分類の問題設定の場合、クラス 0 を予測できる確率、クラス 1 を予測できる確率、
クラス 2 を予測できる確率を出力する。
※ 二値分類をクラス数だけ行う

最終的にそれぞれのクラスに対し確率を出力しているので、最も確率の高いクラスを予測値として採用する。

### 特徴

|項目|説明|
|:--|:--|
|強み|説明能力が高い。入力変数の重要度、オッズ比がわかる。|
|弱み|線形分類器のため、複雑な問題設定に対応できない場合がある。|
|主なハイパーパラメータ||
|C（コストパラメータ）|誤った予測に対するペナルティ。大きすぎると過学習を起こす。|
|penalty|正則化を行う方法を決定する。L1、L2 ノルムから選択する。|

### 実装

```
# モデルの定義
from sklearn.linear_model import LogisticRegression
log_reg = LogisticRegression(C=1.0)

# モデルの学習
log_reg.fit(x_train, t_train)

# モデルの検証
print('train score : ', log_reg.score(x_train, t_train))
print('test score : ', log_reg.score(x_test, t_test))

# 重みの形
# (n_classes, n_features) に対応し、それぞれのクラスを予測する際のそれぞれの特徴量の重みを表す
print('重みの形 : ', log_reg.coef_.shape)

# 重み（係数）
print('重み（係数）: ', log_reg.coef_)

# 切片
print('切片 : ', log_reg.intercept_)

# それぞれの重みを確認(可視化)
fig = plt.figure(figsize=(7, 15))

for i in range(len(log_reg.coef_)):
    ax = fig.add_subplot(3, 1, i+1)
    ax.barh(y=dataset.feature_names, width=log_reg.coef_[i])
    ax.set_title('Class {} '.format(i))

# 各オッズ比を確認
fig = plt.figure(figsize=(7, 15))

for i in range(len(log_reg.coef_)):
    ax = fig.add_subplot(3, 1, i+1)
    odds_ratio = np.exp(log_reg.coef_[i])
    ax.barh(y=dataset.feature_names, width=odds_ratio)
    ax.set_title('Class {} '.format(i))

# カテゴリ 0 の場合
print('重み(係数):',log_reg.coef_[0])
print('オッズ比:',  np.exp(log_reg.coef_[0]))

# 目標値の取得
# scikit-learn を用いて推論を行う際の入力変数は行列である必要がある
print('目標値 : ', log_reg.predict([x_test[0]]))

# 各カテゴリに対する確率の確認
# scikit-learn を用いて推論を行う際の入力変数は行列である必要がある
print('各カテゴリに対する確率 : ', log_reg.predict_proba([x_test[0]]))
```

```
train score :  0.9809523809523809
test score :  0.9777777777777777
重みの形 :  (3, 4) # 3クラス,四つの特徴がある
重み（係数）:  [[-0.39765327  0.8342231  -2.28943702 -0.97842682]
 [ 0.5445833  -0.29081813 -0.23264797 -0.65833762]
 [-0.14693003 -0.54340497  2.52208499  1.63676445]]
切片 :  [  8.99755733   1.54386287 -10.5414202 ]

目標値 :  [2]
各カテゴリに対する確率 :  [[1.31706500e-04 5.98487863e-02 9.40019507e-01]]
```

![スクリーンショット 2022-10-17 5 59 01](https://user-images.githubusercontent.com/20691160/196058024-8f8b3281-0fb0-4886-9d25-7cdd259a6eda.png)

### オッズ比
ビジネスの現場では、結果からどの特徴量が結果にどれくらい影響しているのかを定量評価したいことがほとんど。  ロジスティック回帰では`オッズ比`を用いて、出力変数に対する各入力変数の影響の大きさを確認できる。

オッズ比は、`ある事象の 1 つの群と 1 つの群におけるオッズの比`として定義されている。

* オッズ比が 1 => 事象の起こりやすさが同じこと
* 1 よりも大きい（小さい）とオッズ A がオッズ B よりも起こりやすい（起こりにくい）ということ
* 各入力変数が出力変数に与える影響の大きさを比較することが可能
* オッズ比の値が大きいほど、その入力変数によって出力変数が大きく変動することを意味しており、これを影響の大きさとする
* オッズ比は主に`w`に対してexp(w)をとることで求める -> `np.exp`を使う


![スクリーンショット 2022-10-17 6 09 25](https://user-images.githubusercontent.com/20691160/196058445-3ee8adfb-c350-48cd-a157-ea4747f539e8.png)

```
# カテゴリ0に対する重みとオッズ比
重み(係数): [-0.39765327  0.8342231  -2.28943702 -0.97842682]
オッズ比: [0.67189495 2.30302414 0.10132349 0.37590199]
```

上記から`sepal width が 1 増えるとカテゴリ 0 に当てはまる確率が約 2.30 倍になる`とも表現できる。

このように入力変数と出力変数の関係性を捉えることができることもロジスティック回帰の大きな特徴。

### 予測
* 識別モデル --- 確率を予測できるもの
* 識別関数 --- 確率を予測できないもの

ロジスティック回帰は`識別モデル`に該当し、`predict() メソッド`で推論を行い分類結果を取得でき、`predict_proba() メソッド`で確率を取得することができる。

```
目標値 :  [2]
各カテゴリに対する確率 :  [[1.31706500e-04 5.98487863e-02 9.40019507e-01]]
```
上記では、確かにカテゴリ 2 に対する確率が最も高いことが確認できる。

## ランダムフォレスト
ランダムフォレストは、決定木を複数組み合わせて 1 つのモデルとみなしたアンサンブル学習と呼ばれる手法の 1 つ。

アンサンブル学習には、以下の3つの分類がある。

* バギング(ランダムフォレストはこれ)
* ブースティング
* スタッキング

* `バギング` -> 1 つのデータ群から、ランダムに何割かのデータを復元抽出し、それぞれのデータで学習を行い、結果を統合して最終的な出力とする手法
    * 分類問題の場合は多数決、回帰問題の場合は平均値をとるなどして結果を 1 つにまとめる
* `復元抽出` -> 1 度データを取り出した後、2 回目にデータを取り出す際に、1 回目で取り出したデータをもとに戻してから抽出する
* `弱学習器` -> データのサブセット(分割したデータ)で学習されたモデル

`バギングは、同じデータ群から抽出しているとはいえ、弱学習器それぞれが異なるデータで学習しているため、多様性のある弱学習器の集団であるとみなせる。`

### 特徴
|項目|説明|
|:--|:--|
|強み|比較的良い精度を出す傾向がある。|
|弱み|決定木と比較して可読性が弱い。|
|主なハイパーパラメータ||
|n_estimators|弱学習器の数を決定する|

※ ランダムフォレストは決定木ベースのアルゴリズムであるため、決定木のハイパーパラメータも同様に設定可能

### 実装

```
# モデルの定義
from sklearn.ensemble import RandomForestClassifier
model = RandomForestClassifier(n_estimators=3, max_depth=2, random_state=0)

# モデルの学習
model.fit(x_train, t_train)

# モデルの検証
print('train score : ', model.score(x_train, t_train))
print('test score : ', model.score(x_test, t_test))

# 特徴量重要度の確認
print(' 特徴量重要度 : ', model.feature_importances_)

# 可視化
y = feature_names
width = model.feature_importances_

# 横向きで表示
plt.barh(y=y, width=width);
```

```
train score :  0.9523809523809523
test score :  0.9555555555555556
特徴量重要度 :  [0.18666585 0.         0.32209545 0.4912387 ]
```

<img width="451" alt="スクリーンショット 2022-10-18 5 42 55" src="https://user-images.githubusercontent.com/20691160/196279394-ce3d574c-151f-4511-b639-234cfc6fb437.png">

### XGBoost
XGBoostは、`ブースティング手法`であり、逆学習機を直列でつなぐ。

`1 つ目の弱学習器を学習 -> 1 つ目の学習器の結果を反映 -> 2 つ目の逆学習機を学習`

* バギング
  - 並列処理が可能
  - ブースティングと比較して学習時間が短め

* ブースティング
  - バギングと比較して学習に時間がかかる
  - 精度はバギングよりも高く出る傾向にある

* 勾配ブースティング決定木
  - ブースティングの弱学習器に決定木を使用したものの中でも、1 つ前の弱学習器の予測の残差を次の学習器で小さくするようにして、前の結果を反映させること
  - XGBoost や LightGBM は、勾配ブースティングをさらに、高性能化、高速化したもの
  - テーブルデータにおいてはこれらが他のアルゴリズムに比べて汎用的に良い精度が出る傾向がある

### 特徴
|項目|説明|
|:--|:--|
|強み|精度が高い。欠損値をそのまま扱うことができる。|
|弱み|決定木と比較して可読性が低い|
|主なハイパーパラメータ||
|n_estimators|弱学習器の数を決定する|
|subsample|ランダムに抽出するサンプルの割合|

### 実装

```
# モデルの定義
from xgboost import XGBClassifier
model = XGBClassifier()

# モデルの学習
model.fit(x_train, t_train)

# モデルの検証
print('train score : ', model.score(x_train, t_train))
print('test score : ', model.score(x_test, t_test))

# 可視化
y = feature_names
width = model.feature_importances_

# 横向きで表示
plt.barh(y=y, width=width);
```

```
train score :  1.0
test score :  0.9777777777777777
特徴量重要度 :  [0.02192196 0.03536973 0.7743051  0.16840316]
```

<img width="462" alt="スクリーンショット 2022-10-18 5 56 19" src="https://user-images.githubusercontent.com/20691160/196281657-ae197a6e-d671-4aab-88f7-ab6a95828492.png">

## 分類の評価方法
学習済みモデルを評価する指標には様々なものがある。

* Accuracy (正解率)
* Precision (適合率)
* Recall (再現率)
* F1score (F値)

* `混同行列(こんどうぎょうれつ)` - 予測/予想の正解の組み分けを表にまとめたもの(以下、`正例をがん患者、負例を健康患者`)
* TP (True Positive、真陽性)：予測値を正例として、その予測が正しい場合の数(例: がんと診断され、実際がんだった)
* FP (False Positive、偽陽性)：予測値を正例として、その予測が誤りの場合の数(例: がんと診断されたが、実際は健康だった)
* TN (True Negative、真陰性)：予測値を負例として、その予測が正しい場合の数(例: 健康と診断され、実際健康だった)
* FN (False Negative、偽陰性)：予測値を負例として、その予測が誤りの場合の数(例: 健康と診断されたが、実際はがんだった)

### Accuracy（正解率）
Accuracyは、全ての値を足し合わせて、実際にどれだけ合っているのか測る。

`Accuracy = TP + TN / TP + FP + TN + FN`
※ 全体から診断結果が正しいものの合計を割る

### Precision（適合率）
Precision は、正例（がん）と予測したもののうち、本当に正しく診断できた数の割合を表す
※ 誤診を少なくしたい場合は Precision を重視

`Precision = TP / TP + FP`
※ 正例 (がん) 予測の合計から実際にガン患者の人数を割る

### Recall（再現率）
Recall は、実際の状態が正例（がん）のうち、どの程度正例であると予測できた数の割合。
※ 誤診は許容するが、正例の見逃しを避けたい場合に Recall を重視

`Recall = TP / TP + FN`
※ 全体からがん患者の割合を知ることが可能

### F1 score（F 値）

F1 score（F 値）は、Precision と Recall の両者のバランスを取るために調和平均で計算される指標。

* Precision と Recall は互いにトレードオフの関係ある
  - どちらかの値を上げようとすると、もう一方の値が下がる
  - どちらかの指標を考慮しなければ、もう片方を 1 に近づけることができる少し極端な評価指標

`F1 score = 2 * Recall * Precision / Recall + Precision`

### ROC曲線
分類の指標としてよく用いられるものには ROC 曲線もある。

* `True Positive Rate(真陽性率）= TP / TP + FN` (がん患者全体をがんと診断され、実際にがんだった患者で割る) -> どれだけ正例を発見できているか表す
* `False Positive Rate(偽陽性率）= FP / FP + TN` (健康患者全体をがんと診断され、実際には健康だった患者で割る) -> どれだけ誤った予測をしているのか表す

横軸に FPR を, 縦軸に TPR をとり、`分類の閾値を変化させていった時の各値をプロットした点を結ぶと ROC 曲線を描くことができる`。

ROC は、分類の閾値を課題に応じて最適に調整したい場合に用いられる。

### AUC
AUC（Area of Under a Curve）は曲線の下のエリアを指す用語。

基本的に機械学習領域では ROC/AUC として、Area of Under an ROC Curveを指すことが多い。

良い分類ができた時には 1 に近づく性質があり、AUC の値もモデルの評価に使用することが可能。

## 不均衡データの対策
<対策>
* データ数を増やす
* 推論の確率に応じて、分類する閾値を調整する
* データ数が少ないラベルに対して、重みを増やす（多いラベルに対して、減らす）
* データ数が多いラベルのデータ数を減らす（DownSampling）
* データ数が少ないラベルのデータ数をカサ増しする（OverSampling）
  - [補足](https://colab.research.google.com/drive/1cCM68hp0lGNk_yO98uArdfXVYttgVn1c?usp=sharing)
