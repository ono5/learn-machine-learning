# 分類
分類は、カテゴリが異なる複数のデータを見分けることができる境界線を求めることが目的。

二次元平面上にあるデータ（集合）を一本の直線で分けられることを線形分離可能といい、
アルゴリズムを線形分類器と呼ぶ。

なお、線形ではない形で分類するアルゴリズムを非線形分類器と呼ぶ。

## 決定木(非線形分類器)
決定木の score() メソッドで表示される値には`正解率 （Accuracy）`が用いられる。
(分類の場合は、決定係数が求まる)

例えば、100 個の要素に対して分類を行い 90 個の予測を正しく行えた場合、
正解率は 90%(0.9) になる。

そのため最小の値は 0 となり、最大値は 1 です。

また、回帰では平均二乗誤差が用いられるが、分類では交差エントロピーが主に用いられる。

### 実装
```
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import load_iris

# データセットの読み込み
dataset = load_iris()
x = dataset.data
t = dataset.target
feature_names = dataset.feature_names

# 学習データとテストデータに分割
from sklearn.model_selection import train_test_split
x_train, x_test, t_train, t_test = train_test_split(x, t, test_size=0.3, random_state=0)

# モデルの定義
from sklearn.tree import DecisionTreeClassifier
dtree = DecisionTreeClassifier(random_state=0)

# モデルの学習
dtree.fit(x_train, t_train)

# モデルの検証
print('train score : ', dtree.score(x_train, t_train))
print('test score : ', dtree.score(x_test, t_test))

# 推論
dtree.predict(x_test)
```

## 特徴
[決定木](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier)は木のような構造を用いて、回帰・分類を行うアルゴリズム。

データの標準化や外れ値除去に関しては決定木では不要であるなどメリットがある。


|項目|説明|
|:--|:--|
|強み|解釈が容易。必要な前処理が少ない。|
|弱み|過学習になる場合が多く、汎用性の低いモデルになる傾向がある。|
|主なハイパーパラメータ||
|max_depth(木構造の深さの上限)|過学習を抑えるためのハイパーパラメータ。上限が低いとモデルの表現力は低下し、過学習を抑える。|
|min_samples_split(木構造の分岐先の値)|分岐先のノード数の最低値を設定するハイパーパラメータ。過学習に陥る可能性が上がるので調整が必要。|

### 木構造の書き出し

```
# 木構造の書き出し
import graphviz
from sklearn.tree import export_graphviz
dot_data = export_graphviz(dtree)

# 木構造の表示
graph_tree = graphviz.Source(dot_data)
graph_tree
```
<img width="732" alt="スクリーンショット 2022-10-16 21 26 00" src="https://user-images.githubusercontent.com/20691160/196035346-0971ac0a-3701-4fb6-8da2-01447c09b240.png">

gini という表記はジニ係数を意味しており、分岐されたノードの不純度を表す。
※ もしも、カテゴリが 1 つしか存在しない場合、ジニ係数は 0。

### 入力変数の影響度
アルゴリズムの特性からどの入力変数の影響度が高いかを知ることが可能。

入力変数の影響度を知るには、`feature_importances_`を確認する。

```
# feature importance
feature_importance = dtree.feature_importances_
feature_importance
```

### 可視化
可視化を行うことで、入力変数の影響が大きいかチェックできて便利.

```
# 可視化
y = feature_names
width = feature_importance

# 横向きで表示
plt.barh(y=y, width=width);
```

<img width="464" alt="スクリーンショット 2022-10-16 21 32 31" src="https://user-images.githubusercontent.com/20691160/196035561-d9a5662d-4748-495c-9f7c-36daaed00558.png">

 ↑ `petal width (cm)`の影響度が高い

## サポートベクトルマシン(SVM)
サポートベクトルマシンは、2 つのカテゴリを識別する分類器であり、サポートベクターマシンとも呼ばれる。

![スクリーンショット 2022-10-16 21 56 04](https://user-images.githubusercontent.com/20691160/196036625-e0c61f29-1ff7-4ba1-971e-33a46d8b64e3.png)

1. 上記のように未知のデータを求めたい時、青丸と緑三角の中間地点に境界線を引く(サポートベクトル)
2. 境界に近い丸と三角のデータのマージン(距離)が最大になるように線を引く(マージン最大化)


### 実装
サポートベクトルマシンで学習を行う前にデータの`標準化(スケール合わせ)`が必要.

```
# モデルの定義
from sklearn.svm import SVC
svc = SVC()

# モデルの学習
svc.fit(x_train, t_train)

# モデルの検証
print('train score : ', svc.score(x_train, t_train))
print('test score : ', svc.score(x_test, t_test))

# データの標準
# SVMはデータに対して標準化を行う必要がある
# 標準化とは例えば一つのデータの単位がcmだった時他のパラメータのスケールも同じように一致させる
from sklearn.preprocessing import StandardScaler
std_scaler = StandardScaler()

# 標準化の計算式
# ^      _
# X = (x-x) / σ
# 学習(fitで標準化を行うためのデータセットの平均と標準偏差を算出)
std_scaler.fit(x_train)

# 標準化(transformにて、算出した平均と標準偏差から実際にデータセットの値を変換)
x_train_std = std_scaler.transform(x_train)
x_test_std = std_scaler.transform(x_test)

# 平均
print('平均 : ', round(x_train_std.mean()))

# 標準偏差
print('標準偏差 : ', round(x_train_std.std()))

# モデルの定義
svc_std = SVC()

# モデルの学習(標準化したデータを使う)
svc_std.fit(x_train_std, t_train)

# モデルの検証
print('train score : ', svc.score(x_train, t_train))
print('test score : ', svc.score(x_test, t_test))

print('train score scaling : ', svc_std.score(x_train_std, t_train))
print('test score scaling : ', svc_std.score(x_test_std, t_test))
```

```
train score :  0.9714285714285714
test score :  0.9777777777777777
平均 :  0
標準偏差 :  1
train score :  0.9714285714285714
test score :  0.9777777777777777
train score scaling :  0.9714285714285714
test score scaling :  0.9777777777777777
```

### 特徴

|項目|説明|
|:--|:--|
|強み|未知のデータへの識別性能が比較的強い。ハイパーパラメータの数が少ない。|
|弱み|学習する際に必ずデータの標準化（もしくは正規化）を行う必要がある。|
|主なハイパーパラメータ||
|C（コストパラメータ）|誤った予測に対するペナルティ。大き過ぎると過学習を起こす。|
|gamma（ガンマ）|モデルの複雑さを決定する。値が大きくなるほどモデルが複雑になり過学習を起こす。|

## ロジスティック回帰
ロジスティック回帰 (Logistic regression) はあるデータがカテゴリに属する確率を予測する。

ロジスティック回帰には[シグモイド関数(活性化関数)](https://ja.wikipedia.org/wiki/%E3%82%B7%E3%82%B0%E3%83%A2%E3%82%A4%E3%83%89%E9%96%A2%E6%95%B0)が使われ、あらゆる入力値を0.0～1.0の範囲の数値に変換して出力する。

ロジスティック回帰は入力変数から出力変数に対して二値分類をおこなうモデルであり、
3 クラス分類の問題設定の場合、クラス 0 を予測できる確率、クラス 1 を予測できる確率、
クラス 2 を予測できる確率を出力する。
※ 二値分類をクラス数だけ行う

最終的にそれぞれのクラスに対し確率を出力しているので、最も確率の高いクラスを予測値として採用する。

### 実装

```
# モデルの定義
from sklearn.linear_model import LogisticRegression
log_reg = LogisticRegression(C=1.0)

# モデルの学習
log_reg.fit(x_train, t_train)

# モデルの検証
print('train score : ', log_reg.score(x_train, t_train))
print('test score : ', log_reg.score(x_test, t_test))

# 重みの形
# (n_classes, n_features) に対応し、それぞれのクラスを予測する際のそれぞれの特徴量の重みを表す
print('重みの形 : ', log_reg.coef_.shape)

# 重み（係数）
print('重み（係数）: ', log_reg.coef_)

# 切片
print('切片 : ', log_reg.intercept_)

# それぞれの重みを確認(可視化)
fig = plt.figure(figsize=(7, 15))

for i in range(len(log_reg.coef_)):
    ax = fig.add_subplot(3, 1, i+1)
    ax.barh(y=dataset.feature_names, width=log_reg.coef_[i])
    ax.set_title('Class {} '.format(i))

# 各オッズ比を確認
fig = plt.figure(figsize=(7, 15))

for i in range(len(log_reg.coef_)):
    ax = fig.add_subplot(3, 1, i+1)
    odds_ratio = np.exp(log_reg.coef_[i])
    ax.barh(y=dataset.feature_names, width=odds_ratio)
    ax.set_title('Class {} '.format(i))

# カテゴリ 0 の場合
print('重み(係数):',log_reg.coef_[0])
print('オッズ比:',  np.exp(log_reg.coef_[0]))

# 目標値の取得
# scikit-learn を用いて推論を行う際の入力変数は行列である必要がある
print('目標値 : ', log_reg.predict([x_test[0]]))

# 各カテゴリに対する確率の確認
# scikit-learn を用いて推論を行う際の入力変数は行列である必要がある
print('各カテゴリに対する確率 : ', log_reg.predict_proba([x_test[0]]))
```

```
train score :  0.9809523809523809
test score :  0.9777777777777777
重みの形 :  (3, 4)
重み（係数）:  [[-0.39765327  0.8342231  -2.28943702 -0.97842682]
 [ 0.5445833  -0.29081813 -0.23264797 -0.65833762]
 [-0.14693003 -0.54340497  2.52208499  1.63676445]]
切片 :  [  8.99755733   1.54386287 -10.5414202 ]

目標値 :  [2]
各カテゴリに対する確率 :  [[1.31706500e-04 5.98487863e-02 9.40019507e-01]]
```

![スクリーンショット 2022-10-17 5 59 01](https://user-images.githubusercontent.com/20691160/196058024-8f8b3281-0fb0-4886-9d25-7cdd259a6eda.png)

### オッズ比
ビジネスの現場では、結果からどの特徴量が結果にどれくらい影響しているのかを定量評価したいことがほとんど。  ロジスティック回帰では`オッズ比`を用いて、出力変数に対する各入力変数の影響の大きさを確認できる。

オッズ比は、`ある事象の 1 つの群と 1 つの群におけるオッズの比`として定義されている。

* オッズ比が 1 => 事象の起こりやすさが同じこと
* 1 よりも大きい（小さい）とオッズ A がオッズ B よりも起こりやすい（起こりにくい）ということ
* 各入力変数が出力変数に与える影響の大きさを比較することが可能
* オッズ比の値が大きいほど、その入力変数によって出力変数が大きく変動することを意味しており、これを影響の大きさとする
* オッズ比は主に`w`に対してexp(w)をとることで求める -> `np.exp`を使う


![スクリーンショット 2022-10-17 6 09 25](https://user-images.githubusercontent.com/20691160/196058445-3ee8adfb-c350-48cd-a157-ea4747f539e8.png)

```
重み(係数): [-0.39765327  0.8342231  -2.28943702 -0.97842682]
オッズ比: [0.67189495 2.30302414 0.10132349 0.37590199]
```

上記から`sepal width が 1 増えるとカテゴリ 0 に当てはまる確率が約 2.30 倍になる`とも表現できる。

このように入力変数と出力変数の関係性を捉えることができることもロジスティック回帰の大きな特徴。

### 予測
* 識別モデル --- 確率を予測できるもの
* 識別関数 --- 確率を予測できないもの

ロジスティック回帰は`識別モデル`に該当し、`predict() メソッド`で推論を行い分類結果を取得でき、`predict_proba() メソッド`で確率を取得することができる。

```
目標値 :  [2]
各カテゴリに対する確率 :  [[1.31706500e-04 5.98487863e-02 9.40019507e-01]]
```
上記では、確かにカテゴリ 2 に対する確率が最も高いことが確認できる。

### 特徴

|項目|説明|
|:--|:--|
|強み|説明能力が高い。入力変数の重要度、オッズ比がわかる。|
|弱み|線形分類器のため、複雑な問題設定に対応できない場合がある。|
|主なハイパーパラメータ||
|C（コストパラメータ）|誤った予測に対するペナルティ。大きすぎると過学習を起こす。|
|penalty|正則化を行う方法を決定する。L1、L2 ノルムから選択する。|
