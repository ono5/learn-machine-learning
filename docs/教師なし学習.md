# 教師なし学習
https://colab.research.google.com/drive/1I_oJXn8LJzd_Ji6kk1GDiNwgytLg6I_7#scrollTo=ZHXP4pi0UNId

* 教師あり学習 --- 入力値と目標値をセットで持つ問題設定
* 教師なし学習 --- 目標値を持たない問題設定

教師なし学習人は、主に以下の方法がある。

* 主成分分析 (Principal Component Analysis)
* k- 平均法 (k-means)

## 主成分分析 (Principal Component Analysis)
主成分分析は、`次元削減` の手法。

* 可視化したいときに用いる手法
* 高次元（多次元）のデータを低次元化する手法

`次元削減`とは、例えば 4 次元のデータ（列数が 4 つのデータ）があった場合、2 次元(列数が 2 つのデータ）などの低次元に落とし込むことを指す。

なお、次元削減は単にデータを削除するのではなく、`可能な限り元のデータの情報を保持したまま、低次元のデータに変形を行う`。

また、モデルの学習では主成分分析を適用するために必要な`分散`を算出する。

教師なし学習は正解となる指標がない -> データを低次元でも十分に表すことのできる方向を見つける必要がある -> 主成分分析では`実測値の分散`を用いて求める

* 第一主成分 --- 分散が最大の軸
* 第二主成分 --- 分散の大きさが二番目の軸

```
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris

# データの読み込み
dataset = load_iris()
x = dataset.data
t = dataset.target
feature_names = dataset.feature_names

# データの確認(4次元のデータ)
print(pd.DataFrame(x, columns=feature_names).head())

#  モデルの定義
from sklearn.decomposition import PCA
# n_components->次元削減後の次元数(4次元から2次元へ)
pca = PCA(n_components=2, random_state=0)

# モデルの学習
# 分散の算出
pca.fit(x)

# 分散の確認
print('分散: ', pca.get_covariance())

# 主成分分析の適用
x_transformed = pca.transform(x)

# 主成分分析適用後のデータの確認
print(pd.DataFrame(x_transformed, columns=['第一主成分', '第二主成分']).head(10))

# 1 列目 -> 第一主成分、2 列目 -> 第二主成分
# それぞれの列は次元削減前の情報を保持
# それぞれの列が保持する元のデータの情報の割合を寄与率
# 以下の結果の場合、元のデータを97%保持するということ
# 第一主成分の寄与率：0.9246187232017271
# 第二主成分の寄与率：0.05306648311706782
print('第一主成分の寄与率：{}'.format(pca.explained_variance_ratio_[0]))
print('第二主成分の寄与率：{}'.format(pca.explained_variance_ratio_[1]))

# 0, 1, 2 の 3 つのクラスがあることを確認
# アヤメの花の種類が 3 種類のデータセットを使用
print(np.unique(t))
# 次元削減後のデータを可視化
sns.scatterplot(x_transformed[:, 0], x_transformed[:, 1],
             hue=t, palette=sns.color_palette(n_colors=3));

# 可視化
iris_loading = pca.components_ * np.c_[np.sqrt(pca.explained_variance_)]
df_iris_loading = pd.DataFrame(iris_loading, index=[f'PC{x+1}' for x in range(len(iris_loading))], columns=feature_names)
sns.heatmap(df_iris_loading.head(3), vmax=1.0, center=0.0, vmin=-1.0, square=True, annot=True, fmt='.2f');
```

```
 sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)
0                5.1               3.5                1.4               0.2
1                4.9               3.0                1.4               0.2
2                4.7               3.2                1.3               0.2
3                4.6               3.1                1.5               0.2
4                5.0               3.6                1.4               0.2
分散:  [[ 0.67918961 -0.03571514  1.2714061   0.53137208]
 [-0.03571514  0.18303922 -0.32672469 -0.13706322]
 [ 1.2714061  -0.32672469  3.12237957  1.28464626]
 [ 0.53137208 -0.13706322  1.28464626  0.58834865]]
      第一主成分     第二主成分
0 -2.684126  0.319397
1 -2.714142 -0.177001
2 -2.888991 -0.144949
3 -2.745343 -0.318299
4 -2.728717  0.326755
5 -2.280860  0.741330
6 -2.820538 -0.089461
7 -2.626145  0.163385
8 -2.886383 -0.578312
9 -2.672756 -0.113774
第一主成分の寄与率：0.9246187232017271
第二主成分の寄与率：0.05306648311706782
[0 1 2]
```

## 標準化
主成分分析を行う際には、必ず`標準化`を行う。

標準化とは、平均を 0、分散（標準偏差）を 1 とする操作のこと。

主成分分析では`平均を 0 にすることが必要な前処理`であるため、施した上で主成分分析を適用する。

```
# 標準化
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
x_scaled = scaler.fit_transform(x)

#  モデルの定義
pca = PCA(n_components=2, random_state=0)

# 主成分分析の適用(写像を行う)
x_std_transformed = pca.fit_transform(x_scaled)

# 主成分分析適用後のデータの確認
print(pd.DataFrame(x_std_transformed, columns=['第一主成分', '第二主成分']).head(10))

# 寄与率
print('第一主成分の寄与率：{}'.format(pca.explained_variance_ratio_[0]))
print('第二主成分の寄与率：{}'.format(pca.explained_variance_ratio_[1]))

# 次元削減後のデータを可視化
sns.scatterplot(x_std_transformed[:, 0], x_std_transformed[:, 1],
             hue=t, palette=sns.color_palette(n_colors=3));

fig = plt.figure(figsize=(7, 10))

# 標準化適用前
ax1 = fig.add_subplot(2, 1, 1)
sns.scatterplot(x_transformed[:, 0], x_transformed[:, 1],
             hue=t, palette=sns.color_palette(n_colors=3));
ax1.set_title('Before')

# 標準化適用後
ax2 = fig.add_subplot(2, 1, 2)
sns.scatterplot(x_std_transformed[:, 0], x_std_transformed[:, 1],
             hue=t, palette=sns.color_palette(n_colors=3));
ax2.set_title('After');
```

```
      第一主成分     第二主成分
0 -2.264703  0.480027
1 -2.080961 -0.674134
2 -2.364229 -0.341908
3 -2.299384 -0.597395
4 -2.389842  0.646835
5 -2.075631  1.489178
6 -2.444029  0.047644
7 -2.232847  0.223148
8 -2.334640 -1.115328
9 -2.184328 -0.469014
第一主成分の寄与率：0.7296244541329987
第二主成分の寄与率：0.2285076178670178
```

## k- 平均法 (k-means)

k- 平均法は、クラスタリングと呼ばれる手法に当たり、データを複数のクラスタ（グループ）に分けて大まかな特徴を捉える際に利用する。

<ステップ>
1. 人間側がクラスタ（グループ）の数を決める
2. ランダムに振られた点（重心）から近いものをクラスタとする
3. 紐づいたクラスタとの距離を元に重心を移動させる

```
# データセットの準備
df = pd.read_csv('convinience_store.csv')
df.head(3)

# k-平均法の実装
from sklearn.cluster import KMeans

# k-平均法では予め分割するクラスター数を決めておく(n_clusters)
kmeans = KMeans(n_clusters=3, random_state=0)
kmeans.fit(x) # 学習

# クラスターの中心座標の確認
print(kmeans.cluster_centers_)
print(kmeans.cluster_centers_.shape)

# 入力値に対して、クラスタリングの適用
cluster = kmeans.predict(ｘ)
print(cluster)

# データフレームの作成
df_cluster = df.copy() # データフレームをコピー
df_cluster['cluster'] = cluster
```
